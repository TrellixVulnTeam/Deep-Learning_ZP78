
Objetivo Atual (treinar RNN em TF){

	Necessário{
		Adaptar_base_de_dados{

			Descobrir como fazer o batchpadding na base de comentários do reddir. Dividir em batchs ou usar o conjunto todo? -> 
			(Não precusa de batch. Irei utilizar cada exemplo individualmente tal como no GRU3.0. ) 
			
			Qual a frase de maior tamanho? Como utilizar dynamic shape para o tamanho das frases em um rnn tal como no exemplo do blog.metaflow.fr? ->
			(O tamanho variável das frases pode ser tratado como o próprio batch variável. As palavras seriam os exemplos dentro do batch. Dessa forma
			não é preciso realizar o batch padding, pois todos os exemplos (palavras) seriam unidimensionais denttro de um batch (frase))

			Que erro é esse "Dimension must be 2 but is 3 for 'trans...'" que está dando quando eu faço o mesmo exemplo do lstm teste com 
			shape=[None, 1]? Missão: Resolver este problema para simular o funcionamento da analogia Batch-Frase/Exemplo-Palavra. ->
			(o problema era que tinha que declarar o shape com 3 dimensões...A analogia Batch-Frase/Exemplo-Palavra foi descartada. Acredito que agora
			o melhor caminho é tentar trabalhar com valores dinâmicos de tempo máximo do exemplo e do batch. O valor da terceira coordenada, que representa a entrada isolada, será igual a 1 ou 8000 se for utilizada hot vectors como em um dicionario de 8000 palavras ) 

			Próximo passo: prosseguir normalmente com o exemplo trabalhando com valores dinâmicos. Dúvida de percurso: como simular tf.variables com shapes dinâmicos? No caso,
			a tf.variable b, representando o bias, não pode ser inicializada dinamicamente. Deverei usar tf.tensors para as variaveis W e b então? Não sei se tem diferença (checar);
			Testar utilizando tf.constant_initializer. SIMULA ESSA PORRA COM tf.zeros() QUE A CONSTRUCTION DO GRAPH DEU CERTO!!!!!!!!->
			(Deu tudo certo, porém, novos problemas surgiram...)

			A soluçãao de utilizar tf.tensors para variaveis Weight e Bias não é viável, pois o método minimize() não irá afetá-las. Tenho que utilizar tf.Variable(), não tem jeito.Esse é o próximo passo. Outro ponto importante é a criação de uma matriz Embedding, tal como na GRU theano, que irá receber como entradas hot_vectors. Porém, devo fazer os hot_vectors multiplicarem diretamente a matriz Embedding, ou devo só utilizar indíces para indexar vetores coluna, tal como no theano?? Oh dúvida cruel...->
			(Problema resolvido. Estou utilizando tf.Variable, com a dimensão do número de exemplos dinamica mesmo. Quanto ao Embedding, também está funcionando. Estou utilizando indices na entrada da mesma forma como no theano.)

			Descobrir o problema que nao está permitindo a multiplicacao do target com o tf.log(tf.clip_by_value(prediction,1e-10,1.0)). Está dizendo que ambos estão em graphs diferentes. Como assim? Como a multiplicação de ambos era possível antes? ->
			erro bobo. Só tinha que reinicia tudo em um novo grago.

			Descobrir a forma certa de se realizar o softmax: devo realizar o softmax na saída flat ou na saída reshaped() com os batchs certos? Acho que tanto faz, mas é bom verificar. Também estou meio intrigado com as funções sparse_softmax_crossentropy_with_logits utilizadas no Wildml. Não sei se serão úteis, mas é bom dar uma olhada também. ->
			Dúvidas desnecessárias. O softmax com ou sem batch o mesmo, porém é muito mais prático com o flat. Qanto ao sparse_softmax_crossentro... ele só faz o softmax e calcula a crossentropia para a saída pura da rede em modo flat utilizando como parametro os targets desejados como índices flat também. Cada valor do target flat é interpretado como o índice correto da linha da saída softmaxI(), ou seja, faz o que a crossentropia faz.

			Proseguir normalmente após o cálculo do crossentropy. 

			Ok...a rede treinou, finalmente! Porém, um exemplo não passou no teste. Trata-se do exemplo número 11320. É uma frase vazia, contendo apenas a palavra inicial que é o token de início e o target final que é o token final. Por que raios ele não passa? Não sei. O problema é no minimize. Consegue calcular até o losses, mas o minimize da pal. Talvez ele não funcione para losses unitários...Se não conseguir solucionar esse problema, a alternativa é simplesmente eliminar exemplos vazios da base de dados ou criar uma exception quando isso ocorrer. Uma boa idéia é testar esse exemplo na rede do theano pra ver o que acontece. -> 
	

			Mistério: Porque a multiplicação do weight ou a saída da rede da resultados uniformes no lstm_teste_Dynamic_Seq e resultados mais discrepantes com valores maiores no loss_masking? 10/fev -> (Mistério ainda não resolvido)

		}
		Adaptação para modelo de classe em python{
			Realizar a "tradução" para do código gru_theano.py para tensorflow ->
			A priori, terminei de adaptar para o modelo de classe. Não há muito mistério, pois a maior parte dos procedimentos em TF são feitos por funções de alta ordem. Deve-se ter atenção para o termo self, que referencia atributos instanciais, por isso self deve sempre ser passado como parametro dentro de uma classe, pois ele representa a própria instancia quando criada. Outra obsv importante é que as "functions() em theano funcionam de forma semelhante ao "sess.run()" do tensorflow, pois elas referenciam as variáveis de entrada e suas respectivas saídas dentro do contexto do Grafo de controle. Por isso foi fácil substituí-las no código. 

			Relizar as mudanças necessárias nos scripts "utils.py" e "train_theano.py" para que rodem e realizem o treinamento em conunto com GRUTensorflow.py ->

			Descobrir porque tf.Session() aparentemente não está funcionando quando chamado dentro de 2 contextos diferentes no Python: o contexto código principoal train_tensorflow e do objeto model. 
			problema trivial resolvido

			Corrigir e adaptar a função generates_sentences. Tente fazer a frase candidata inicial que começa como token de início já começar no formato numpy com shape (1, x, 1) e o append de novas palavras se da ao longo da coord. x. ->
			Feito

			Corrigir bug estranho que ocorre ao tentar printar uma frase já pronta e gerada pela rede com a funç]ao print_sentence de "utils.py"
			corrigido

			Realizar uma encapsularização da função GRU-tensorflow, pois estou tendo que adaptar em todas as outras funçẽos do código a notação de lista para np.array(). O ideal é que a interface em python seja a mesma para ambas as redes, ou seja, mantenha a notação em lista e faça a conversão para np.array dentro da classe GRU-tensorflow, se possível
			Maior parte realizada com sucesso. 

			Consertar as funções save e load model parameters. Save não está coseguindo salvar alguns parametros, como V e da erro ao salvar a sess (será que é necessário salvar sess?). Além disso, temos que adicionar os parametros reais utilizados pela rede que serão salvos, ou seja, weights, bias e as variables internas criadas pela função dynamic_rnn (boa parte já foi feita através do indexamento pelo nome da variável no grafo tf em sess.graph.get_tensor_by_name("NOME") )
			Concluído, mas pode ser ncessário mudar (ver próximo problema)

			Descobrir como carregar os valores de weight e bias da dynamic_rnn de um save previo. Não estou conseguindo pela função Variable().assign(), pois ela só assinala o novo valor a variavel que aponta para o valor dos pesos, não para os pesos em si. 
			Resolvido utilizando função tf.Saver()

			Descobrir se o saver está conseguindo subescrever arquivos existentes de saves antigos. Ageitar o "PrintEvery" para que o modelo printe as frases e salve os parametros mais vezes

		}		
		



		certificar que theano e tensorflow estão otimizados{

			
			Batchpading Theano:
				Realizar o batchpadding no theano e aprender sobre tensorvariables, sharedvariables, eval(), function() e o que mais for necessario 

				colocar a rede para treinar, já está funcionando o esquema de indices aleatorios que indexam x_train e alimentam um batch aleatorio a entrada tensor type do theano. Agora é só colocar a rede para funcionar em uma base de dados real e rezar pra funcionar.

				Ajeitar matriz U e quebrala em 2, 1 para cada camada. Ajeitar as dimensões. Também adaptar o exemplo para receber o conjunto de teste. Boa sorte


				Reproduzir testes razo de desempenho com 1000 interações (ou mais) no GPU01, e descobrir porque diabos o tensorflow 1.3 está 10 x mais rápido que o theano 0.10, porém estourando 95 C enquanto o theano fica em 83C...
			

			Batchpading Tensorflow: 
				Não consigo "printar" o valor de um batch com elementos de extensão variavel. Por que? O tensorflow ficar rodando e nada...Resolver esse problema.
				-> Problema devido ao mal uso de filas e threads

				O principal já foi feito. Já estou dominando o esquema de filas como input de variaveis. Porém, de forma ineficiente. Demora muito pra colocar todos os exemplos em uma única fila. Tenho que tentar utilizar o formato do tensorflow com TFRecord. Eis meu próximo objetivo. 

				-> Aleuluia! TFRecords já em funcionamento, porém só é possível armazenar dados da forma [[a,b,c], [d,f,g], [h,i]]. Deve-se fazer uma conversão para cada elemento desenfilerado para que fique no shape (None, 1). O que pode ser custoso. Porém, eis a solução de desenfileirar e encaminhar vários exemplos para uma fila FIFO com capacidade grande antes do treinamento, como forma de poupar tempo. 

				Tentar resolver o problema de indexar o vetor 0 do embedding, que está como aleatório. Se for possível fazer com que esse vetor seja igual a [0,0,0...0], o problema está resolvido, creio eu. Também tem-se que resolver o problema do masked losses o mais rápido possível.



			}
			Mascarar perdas
			entendimento completo de como "printar" dados de tensor e variable no TF

	}

		keras{

			Algo estranho ocorreu no treinamento com 500k: loss vai pra 1e-7 e acc vai pra 0%. Além disso é necessário colocar o embedding ainda. Vamos entao treinar a rede e printar o resultado de (acc, loss) a cada 100 iterações ou algo assim e ver o que está acontecendo.


		}
	
		Benchmark{

			Analisar a possiblidade de se utilizar o PTB (penn treebank) para avaliar a qualidade das redes

		}

	Concluído{
		leitura da documentação da classe optimizer e minimizer
		entendimento do conceito de lista de variáveis
		Terminar o modelo do "monik"
	}
}

Próximos Objetivos {

	Modificar parâmetros do optimizer diretamente
	Certificar que o modelo em Theano e Tensorflow realizam as mesmas operações

}

